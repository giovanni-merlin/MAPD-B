{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: Introduction to PySpark\n",
    "\n",
    "Distributed computing over a cluster is a widely adopted strategy in the case of many applications both in- and out-side academia.\n",
    "\n",
    "Immagine you are interested in working on a very large dataset, at the scale of hundreds of Terabytes or even Petabytes.\n",
    "As discussed in the Data Management part of the course, a large horizontally scalable storage solution is going to be necessary to host the entire dataset.\n",
    "Even if we assume the storage solution is sorted out, we are left with the issue... how can we deal with all these data in order to produce the \"final result\" we aim at?\n",
    "\n",
    "\n",
    "* Data are distributed over many nodes, and collecting all data in one node (e.g. loading the entire dataset in a single computer memory) would be impossible\n",
    "\n",
    "* Not all data in the entire dataset is necessarily going to be relevant to solve your problem. You may want to first load all data, explore it, filter it and pre-process it\n",
    "\n",
    "* Then, you may want to move to the actual data analysis phase, and possibly to the application machine learning models on the preprocess data. However, even the preprocessed data may not be enough to fit your local machine memory, and once again a distributed system is going to be necessary.\n",
    "\n",
    "What you would like to do is to run the pre-processing on the nodes of the cluster where the data is distributed, and return only the high level quantities that will be used in the analysis, or even the end-result of the entire analysis. \n",
    "\n",
    "This is the core concept of distributed computing frameworks like Hadoop and Spark: the code/execution goes to the data, rather than the data coming to the machine where the code resides (data locality). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker Cluster\n",
    "\n",
    "For this set of lectures we will work with a standalone Spark cluster deployed on your machine via Docker. \n",
    "This is a mock-up of a larger scale cluster setup, and can also be used to develop and test your application before deploying it on a \"production\" cluster.\n",
    "\n",
    "We are using `docker compose` to create the cluster with multiple Docker containers acting as computing nodes.\n",
    "\n",
    "The cluster dashboard is exposed at `localhost:8080` and the master is already available at `spark://spark-master:7077`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Session\n",
    "\n",
    "We can now create the Spark session. \n",
    "\n",
    "With the following command we are asking to the master (and the resource manager) to create an __application__ with the required resources and configurations. \n",
    "\n",
    "In this case we are using all the default options (e.g. number of core and number of executors), but we can also specify them by hand with `.config(\"spark.some.config\", \"value\")`. \n",
    "\n",
    "The list of available configurations can be found [here](https://spark.apache.org/docs/latest/configuration.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/27 07:19:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# import the python libraries to create/connect to a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# build a SparkSession \n",
    "#   connect to the master node on the port where the master node is listening (7077)\n",
    "#   declare the app name \n",
    "#   configure the executor memory to 512 MB\n",
    "#   either *connect* or *create* a new Spark Context\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .appName(\"My first spark application\")\\\n",
    "    .config(\"spark.executor.memory\", \"512m\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the content of the Spark Session `spark` object.\n",
    "\n",
    "This is the entry point to the main Spark functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e88352decd02:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>My first spark application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7991813e50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __Spark UI__ link provided won't work, as it refers to the Docker container running the Spark application.\n",
    "\n",
    "Port mapping is however provided for the `4040` port and you can open the Spark UI link to the application pointing your browser to `localhost:4040`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Spark Session we can access the Spark Context.\n",
    "\n",
    "The Spark Context is the driver application program we will use to submit applications to Spark, and it is used to work with RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e88352decd02:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>My first spark application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://spark-master:7077 appName=My first spark application>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a spark context\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# print its status\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB:best idea in real life applications is to have data already scattered in different worker nodes (spark is a file system?), instead here we are sending all data to headnode (master) and it will subdvide the data by itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelize!\n",
    "\n",
    "The main feauture of Spark is to distribute data in a number of **partitions**, either using the _memory_ or the _disk_ available in the executors.\n",
    "\n",
    "Do also keep in mind the difference between the Worker and the Executor in Spark!\n",
    "\n",
    "The first command we will use is `parallelize()`. \n",
    "\n",
    "This is a function of the `SparkContext` (`sc`) object and it is used to create a Resilient Distributed Datasets (**RDD**) from a list collection. \n",
    "In other words, it takes a collection and split it amongst the executors. \n",
    "\n",
    "We will start with a basic example to get familiar with the main functions of Spark RDDs. \n",
    "\n",
    "Starting from a python \"dataset\" we create an RDD, i.e. a distributed dataset residing on the cluster, `dist_data`, and then operate on it in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple python list, as the starting dataset\n",
    "data = [1,2,3,4,5,6,7,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelize it over the available executors!\n",
    "dist_data = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go and check the Spark web UI\n",
    "- http://localhost:8080 for the Master node web UI\n",
    "- http://localhost:4040 for the SparkContext web UI (where you application will reside)\n",
    "\n",
    "In principle we could expect the application to report something related to our request of parallelizing data over the cluster..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "But instead... nothing happend in the Web UI, even though we asked Spark to parallelize our data into a number of partitions! \n",
    "\n",
    "That's because `parallelize` is not an **action**, but a **transformation**, and thus is *lazy* by nature. \n",
    "\n",
    "PROMISE OF A FUTURE RESULT...\n",
    "\n",
    "We can trigger it by using an action, for instance the `count` action, a function used to count the number of elements in the RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count elements of the rdd\n",
    "dist_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back and check the Spark web UIs and interpret its content..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using two cores, in the WebUI you will see two parallel lines for this task. \n",
    "\n",
    "That's because the RDD consists of two **partitions**. \n",
    "\n",
    "Indeed, under the hood, the RDDs are stored and represented as partitions, \"small blocks\" of the original dataset that will be **unit of parallelism**.\n",
    "\n",
    "<br/><center><img src='imgs/lecture1/rdd_partitions.png'/></center><br />\n",
    "\n",
    "\n",
    "In other words if we have two cores in our worker, but the RDD is composed of one partitions, then only one task at the time will be run. \n",
    "\n",
    "On the other hand, if the partitions are four, only two of them will be processed in parallel.\n",
    "\n",
    "\n",
    "<br/><center><img src='imgs/lecture1/partitions_processing.png'/></center><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split our data in an arbitrary number of partitions we can use `numSlices`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parallelize using a different number of partitions\n",
    "sc.parallelize(data, numSlices=8).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask Spark to count the number of partitions of a given RDD with the method `getNumPartitions()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sc.parallelize() is itself an RDD\n",
    "# thus we can call getNumPartitions() on the RDD created via the parallelize transformation\n",
    "sc.parallelize(data, numSlices=8).getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Spark will try to split the dataset into a number of partitions equal to the number of executors.\n",
    "\n",
    "Not always successfully though... \n",
    "\n",
    "You should make sure what is the number of slices your dataset is parallelized and how it compares with the available computing resources, in order to parallelize your computation effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of partitions of the RDD created previously\n",
    "dist_data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to visualize that **the RDD resides in the worker nodes**, and not in your client/local machine.\n",
    "If we print the `dist_data` object from Jupyter we will not see the actual data contained in the RDD, but only that this is a distributed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274\n"
     ]
    }
   ],
   "source": [
    "# print the dist_data object from here\n",
    "print(dist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually a very important concept, as in a distributed system the datasets we are processing are typically much larger than the availble memory of the driver (it's one of the main reasons we use a distributed system in the first place).\n",
    "\n",
    "_Returning the content of the whole dataset in a single machine is not a clever idea_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, a distributed dataset can be collected in the driver with `collect()`.\n",
    "\n",
    "By issuing this action, **all the workers will send back all the blocks of the RDD**. \n",
    "\n",
    "This is generally used to collect only the end-results, after a computationally heavy processing is first run on the worker nodes, and a slim and manageable result can be retrieved. \n",
    "\n",
    "This function should be **used with caution** since **it fetches the entire RDD to a single machine** and can cause the driver to run out of memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect the distributed dataset (so far, a mere list)\n",
    "dist_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a massive dataset and we are looking for inspecting only a small subset of elements we can use `take(num)`.\n",
    "\n",
    "`take` allows to return only the first `num` number of elements from the RDD, thus limiting the amount of data we are collecting from the cluster.\n",
    "\n",
    "Under the carpet, Spark will look at one partition's data, and will decide if one is enough to satisfy your request, or it will need to retrieve data from more than one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve a few elements from the RDD (3)\n",
    "dist_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map and Reduce\n",
    "\n",
    "We can now write our first map-reduce application. \n",
    "\n",
    "The method `map(f)` will apply a function `f` to each _element_ of the RDD. \n",
    "\n",
    "For instance, let's create a simple function to increment by 1 each of the elements of the RDD.\n",
    "\n",
    "```\n",
    "[1,2,3,4,5,6,7,8] --> [2,3,4,5,6,7,8,9]\n",
    "```\n",
    "\n",
    "The function itself is very simple, we can use a Python `lambda` function to take care of that:\n",
    "\n",
    "```python\n",
    "lambda(x: x+1)\n",
    "```\n",
    "\n",
    "Remember, **this is a transformation**, not an action.\n",
    "Being a `map` phase-equivalent to the Hadoop Map-Reduce, we have the idea that all partitions will be operated in parallel.\n",
    "\n",
    "To trigger the transformation, we can chain it with an action, such as `collect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# increment each value of the array with map\n",
    "# and collect the result\n",
    "dist_data.map(lambda x: x+1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differently from plain Hadoop, in Spark **we can chain together multiple transformations, without having to strictly follow the Map-then-Reduce pattern**.\n",
    "\n",
    "For instance, we can even run an entire chain of map functions one after the other with no reduce function at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's chain a `+1` increment function with a `/2` function to divide the result of the previous  multiple map transformations.\n",
    "\n",
    "We can continue using `map` and `lambda` functions to do this, and chain a `.map()` after other `.map()` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple map transformations\n",
    "# plus collect (action)\n",
    "dist_data.map(lambda x: x+1)\\\n",
    "         .map(lambda x: x/2)\\\n",
    "         .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we can see that we have already moved away from the strict pattern of Map-Reduce \"a-la-Hadoop\", where every operation is a single Map phase followed by a single Reduce phase.\n",
    "\n",
    "In this example we have instead 2 Map phases, and a single Reduce phase (the `collect`).\n",
    "\n",
    "The optimization provided by the DAG scheduler will hide the map functions under the same processing stage, and will take care of running them pipelined as fast as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important operation available is an explicit `reduce` function.\n",
    "\n",
    "Reduce is an **action** that aggregates all the elements of the RDD using some user-provided function, and returns the result to the driver. \n",
    "\n",
    "We will see later on another version of reduce, `reduceByKey`, wich performs the reduction for elements with the same key, and we'll reproduce the wordcount example.\n",
    "\n",
    "For the time being, let's explore a simple `reduce` implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce actions can take a **function as argument**, which will be applied to _all the elements of the RDD_, to perform some kind of **aggregation**.\n",
    "\n",
    "For example, we can think about summing the elements of a RDD in pairs with a lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# increment each element of the dataset (transformation)\n",
    "# and sum all of them in pairs (action)\n",
    "dist_data.map(lambda x: x+1)\\\n",
    "         .reduce(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A schematic view of this simple Map-Reduce application can be seen scketched here:\n",
    "\n",
    "<br/><center><img src='imgs/lecture1/map_reduce_increment.png'/></center><br/>\n",
    "\n",
    "Please note how the reduce function does not necessarily have to complete its task in a single iteration.\n",
    "In this case, the sum in pairs will continue on until we are left with a single scalar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Write a simple reduce function to find the minimum of `dist_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple approach is to evaluate the min of pairs of values `x` and `y`, by reducing the `min(x,y)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce the min function over pairs of values for the dist_data dataset\n",
    "\n",
    "#  each value of the array with map\n",
    "# and collect the result\n",
    "dist_data.reduce(lambda x, y: min(x,y))\n",
    "\n",
    "#built in function properly understood by pyspark\n",
    "dist_data.reduce(min)\n",
    "\n",
    "#\n",
    "dist_data.min()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Create a set of 100 points in a 3D space, rapresented as tuples of randomly-generated floats, and calculate the maximum distance of the points from the origin `(0,0,0)`.\n",
    "\n",
    "1. Create the collection of tuples on the client (Python)\n",
    "2. Parallelize the collection in a Spark RDD\n",
    "3. Use Spark to evaluate the euclidean distance of each point from the origin\n",
    "4. Use Spark reduce to evaluate the maximum distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the collection of tuples on the client (Python)\n",
    "import numpy.random as rnd\n",
    "\n",
    "(x,y,z) = (rnd.uniform.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Parallelize the collection in a Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Use Spark to evaluate the euclidean distance of each point from the origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Use Spark reduce to evaluate the maximum distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should keep in mind that every time we move data **from the client to the cluster** or **from the cluster to the client** we are potentially paying a large price:\n",
    "- moving data from the client to the workers implies a large amount of data transfers, through the driver, and to the executors. <font color='blue'>If we can read data directly form the worker nodes (we'll see later a couple of examples) this should always be preferred</font>.\n",
    "- on top of this issue, moving data from the workers to the client implies an additional risk. We might have more data on the cluster than what we can afford to store locally on our client machine. Moving back data (i.e. doing a `collect`) from the cluster might crash the driver, and crash your application. <font color='blue'>Refrain from asking Spark to `collect` an entire GB-, TB- or even PB-sized dataset to your laptop</font>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - compute $\\pi$\n",
    "\n",
    "Estimate $\\pi$ by simulating random points in the unit square (side length of $1$) and counting how many fall in the unit circle. \n",
    "\n",
    "The probability of one point falling inside the circle is\n",
    "$$\n",
    "P = \\frac{\\text{Area circle}}{\\text{Area square}} = \\frac{\\pi}{4}\n",
    "$$\n",
    "\n",
    "We can estimate this probabiliy by counting the number of simulated points inside the circle\n",
    "$$\n",
    "P = \\frac{\\text{#Points in circle}}{\\text{#Points}} \n",
    "$$\n",
    "\n",
    "Thus obtaining \n",
    "$$\n",
    "\\pi \\approx 4 \\cdot \\frac{\\text{#Points in circle}}{\\text{#Points}}\n",
    "$$\n",
    "\n",
    "<br/><center><img src='imgs/lecture1/pi_estimation.png'/></center><br/>\n",
    "\n",
    "This can be done in Spark running the computation in parallel with the following steps:\n",
    "1. Create a \"dummy RDD\" containing placeholders for all the points (e.g. use the value 0 as placeholder for each data point)\n",
    "2. Generate the points as random (x,y) pairs and check if each of them falls inside/outside the circle\n",
    "3. Count the points inside the circle\n",
    "\n",
    "Think about which transformations are needed to generate the points and to check if they fall inside the circle.\n",
    "Combine all the transformations and the action in a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_points = 1e5 # a reasonable amount, such as 100_000\n",
    "\n",
    "# instantiate a \"placeholder RDD\"\n",
    "points_rdd = np.array([0,0])\n",
    "\n",
    "def in_circle(dummy):\n",
    "    # generate a point randomly and check if\n",
    "    # it is inside the circle \n",
    "    # by returning 0 or 1\n",
    "    u = np.random.uniform(size=2)\n",
    "    \n",
    "\n",
    "# count the number of points inside the circle\n",
    "\n",
    "\n",
    "# print the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/27 07:32:02 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.18.0.3 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1245, in func\n",
      "    initial = next(iterator)\n",
      "  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_13/833367041.py\", line 14, in <lambda>\n",
      "  File \"/tmp/ipykernel_13/833367041.py\", line 9, in in_circle\n",
      "NameError: name 'x' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/04/27 07:32:03 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job\n",
      "23/04/27 07:32:03 WARN TaskSetManager: Lost task 1.3 in stage 0.0 (TID 7) (172.18.0.3 executor 0): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6) (172.18.0.3 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1245, in func\n    initial = next(iterator)\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_13/833367041.py\", line 14, in <lambda>\n  File \"/tmp/ipykernel_13/833367041.py\", line 9, in in_circle\nNameError: name 'x' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1245, in func\n    initial = next(iterator)\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_13/833367041.py\", line 14, in <lambda>\n  File \"/tmp/ipykernel_13/833367041.py\", line 9, in in_circle\nNameError: name 'x' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m     y\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# count the number of points inside the circle\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m points_inside \u001b[38;5;241m=\u001b[39m \u001b[43mpoints_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43min_circle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# print the final result\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(pl \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39mpoints_inside\u001b[38;5;241m/\u001b[39mnum_points)\n",
      "File \u001b[0;32m/usr/bin/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:1250\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[0;32m-> 1250\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vals:\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce(f, vals)\n",
      "File \u001b[0;32m/usr/bin/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1197\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/bin/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6) (172.18.0.3 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1245, in func\n    initial = next(iterator)\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_13/833367041.py\", line 14, in <lambda>\n  File \"/tmp/ipykernel_13/833367041.py\", line 9, in in_circle\nNameError: name 'x' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1245, in func\n    initial = next(iterator)\n  File \"/usr/bin/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_13/833367041.py\", line 14, in <lambda>\n  File \"/tmp/ipykernel_13/833367041.py\", line 9, in in_circle\nNameError: name 'x' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "num_points = 100_000 # a reasonable amount, such as 100_000\n",
    "# instantiate a \"placeholder RDD\"\n",
    "points_rdd =sc.parallelize([0]*num_points, numSlices=8)\n",
    "\n",
    "def in_circle(dummy):\n",
    "    # generate a point randomly and check if\n",
    "    # it is inside the circle \n",
    "    # by returning 0 or 1\n",
    "    x\n",
    "    y\n",
    "    \n",
    "\n",
    "# count the number of points inside the circle\n",
    "points_inside = points_rdd.map(lambda x: in_circle(x))\\\n",
    "                            .reduce(lambda a, b: a+b)\n",
    "\n",
    "\n",
    "# print the final result\n",
    "print(pl = 4*points_inside/num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89225461, 0.13894305])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same result can be achieved using the `filter` transformation. \n",
    "\n",
    "`filter(f)` returns a new RDD containing only the element of source RDD on which `f` is `true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the same exercise using filter\n",
    "points_inside = points_rdd \\\n",
    "    .map(lambda x: random.random()**2 ) \\\n",
    "    .filter(\"\"\" the filter function \"\"\") \\\n",
    "    .count()\n",
    "#points_rdd is not doing much! just say spark that is alive\n",
    "\n",
    "# print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the same exercise using filter\n",
    "points_inside = points_rdd \\\n",
    "    .map(\"\"\" the map function \"\"\") \\\n",
    "    .filter(\"\"\" the filter function \"\"\") \\\n",
    "    .count()\n",
    "#points_rdd is not doing much! just say spark that is alive\n",
    "\n",
    "# print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Reduce by key and flat map\n",
    "\n",
    "Consider the following dataset where each element consists of a tuple `(group, value)` (a simple example of a **key-value pair**). \n",
    "\n",
    "This can be for example:\n",
    "- `group` = product class, and \n",
    "- `value` = revenue\n",
    "\n",
    "Or readings from a set of sensors:\n",
    "- `group` = sensor identifier\n",
    "- `value` = reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = [('group1', 10), \n",
    "              ('group2',  4), \n",
    "              ('group3',  1), \n",
    "              ('group2',  7), \n",
    "              ('group1',  8)]\n",
    "class_rdd = sc.parallelize(class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may be interested in operating only on the _values_ of the dataset, discarding the _keys_.\n",
    "\n",
    "We can write a map function to do this, using the python syntax to access only the `[0]` (the key) or the `[1]` element of the tuple.\n",
    "\n",
    "For instance, we can implement a simple increment of `+1` to all values via a `map` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('group1', 11), ('group2', 5), ('group3', 2), ('group2', 8), ('group1', 9)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# operate only on the values\n",
    "class_rdd.map(lambda el: (el[0], el[1]+1)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same result can be achieved using <font color='blue'>**mapValues**</font>.\n",
    "\n",
    "This allows to write an equivalent `map` function that applies only to the values of the groups disregarding the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('group1', 11), ('group2', 5), ('group3', 2), ('group2', 8), ('group1', 9)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the same using map values\n",
    "class_rdd.mapValues(lambda val: val+1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform a reduce function for each class using <font color='blue'>**reduceByKey**</font>: in this way we are applying the reduce function only to all the elements of the same class. \n",
    "#### --> a similar concept of groupby in pandas \n",
    "\n",
    "Despite its name, `reduceByKey` is not an action, but a transformation, since it returns a distributed dataset (the result of an aggregation by key could still give a very large number of items, if the number of keys is large).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the minimum value of all elements of each group using `reduceByKey` and collecting the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the minimum using reduce by key\n",
    "class_rdd.reduceByKey(\"\"\" compute the minimum per group \"\"\")\\\n",
    "         .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('group1', 8), ('group3', 1), ('group2', 4)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the minimum using reduce by key\n",
    "class_rdd.reduceByKey(min)\\\n",
    "         .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further filter our results by using <font color='blue'>**takeOrdered**</font> to get the first `n` results sorted by a given field.\n",
    "\n",
    "By default, `takeOrdered(n)` takes the first `n` elements of the RDD in ascending order.\n",
    "To get the descending order (or any other arbitrary order) we can specify a function, such as:\n",
    "```python\n",
    "lambda x: -x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the maximum value for each group, and return only the 2 with the largest values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_rdd.reduceByKey(\"\"\" compute the maximum per group \"\"\") \\\n",
    "    .takeOrdered(\"\"\" select the 2 elements with largest value \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('group1', 10), ('group2', 7)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_rdd.reduceByKey(lambda x, y: max(x,y)) \\\n",
    "    .takeOrdered(2, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are not interested in treating the dataset as key-value pairs, or in general we have a non-flat dataset (e.g. each element of the RDD is a list, or another object), we can use `flatMap` to \"explode\" each element returning a plain sequence of elements.\n",
    "\n",
    "This can be extremely useful if you have data stored as arbitrary data structures, but you would like to use them as plain lists of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['group1', 10, 'group2', 4, 'group3', 1, 'group2', 7, 'group1', 8]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten the class rdd\n",
    "class_rdd.flatMap(lambda el: el)\\\n",
    "         .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 2, 3, 4]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list of elements, and flatten them into a single list of items\n",
    "sc.parallelize([[1,2,3], [2,3,4]]) \\\n",
    "    .flatMap(lambda x: x) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Spark every single sub list is an item, therefore need to flatten to individuare the correct items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Exercise 3: word count\n",
    "\n",
    "Let's rerun the word-count example seen in class when discussing the Map-Reduce programming paradigm, starting from the following arbitrary text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three Rings for the Elven-kings under * the sky,Seven for the Dwarf-lords in $ their halls of stone,Nine for Mortal # Men doomed to die,One for [ the Dark Lord on his dark throneIn the Land of _ Mordor where the Shadows lie.\n",
      "One Ring to } rule them all, One @ Ring to find them,One Ring $ to bring them all and in the darkness bind themIn the Land of Mordor * where the Shadows lie.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "    'Three Rings for the Elven-kings under * the sky,',\n",
    "    'Seven for the Dwarf-lords in $ their halls of stone,',\n",
    "    'Nine for Mortal # Men doomed to die,',\n",
    "    'One for [ the Dark Lord on his dark throne',\n",
    "    'In the Land of _ Mordor where the Shadows lie.\\n',\n",
    "    'One Ring to } rule them all, One @ Ring to find them,',\n",
    "    'One Ring $ to bring them all and in the darkness bind them',\n",
    "    'In the Land of Mordor * where the Shadows lie.\\n'\n",
    "]\n",
    "\n",
    "print(''.join(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is counting the occurence of each word using a map-reduce approach in Spark.\n",
    "\n",
    "But first, you may want to need to clean-up the dataset by removing all \"stray\" symbols such as `@` and `$`.\n",
    "\n",
    "To avoid double counting the same word when in lower- or upper-case, you may also want to change all the words to lower case.\n",
    "\n",
    "***Hint***: use `string.punctuation` or a regular expression to remove the unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Three Rings for the Elven-kings under * the sky,',\n",
       " 'Seven for the Dwarf-lords in $ their halls of stone,',\n",
       " 'Nine for Mortal # Men doomed to die,',\n",
       " 'One for [ the Dark Lord on his dark throne',\n",
       " 'In the Land of _ Mordor where the Shadows lie.\\n',\n",
       " 'One Ring to } rule them all, One @ Ring to find them,',\n",
       " 'One Ring $ to bring them all and in the darkness bind them',\n",
       " 'In the Land of Mordor * where the Shadows lie.\\n']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parallelize the message\n",
    "message_rdd = sc.parallelize(message)\n",
    "message_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n",
      "h\n",
      "r\n",
      "e\n",
      "e\n",
      " \n",
      "R\n",
      "i\n",
      "n\n",
      "g\n",
      "s\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "E\n",
      "l\n",
      "v\n",
      "e\n",
      "n\n",
      "-\n",
      "k\n",
      "i\n",
      "n\n",
      "g\n",
      "s\n",
      " \n",
      "u\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "*\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "s\n",
      "k\n",
      "y\n",
      ",\n",
      "23/04/27 09:16:26 ERROR TaskSchedulerImpl: Lost executor 0 on 172.18.0.3: worker lost\n"
     ]
    }
   ],
   "source": [
    "for i in message[0]:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: create the words_rdd where each element is a word\n",
    "# eg: ['one', 'ring', 'to', 'rule', 'them']\n",
    "\n",
    "def clean_row(row):\n",
    "    # this function should:\n",
    "    #   1. clean a row from punctuation \n",
    "    #   2. put each word in lowercase\n",
    "    #   3. and return the list of words\n",
    "    word_list = []\n",
    "    \n",
    "    # your code\n",
    "    for i in row:\n",
    "        if i in string.punctuation: i == \"\"\n",
    "    \n",
    "    \n",
    "    return word_list\n",
    "\n",
    "# apply transformation\n",
    "words_rdd = \"\"\" your transformation \"\"\"\n",
    "\n",
    "# print a few items\n",
    "words_rdd.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: perform word count!\n",
    "# -> crate pairs (word, 1)\n",
    "# -> reduce based on the word and count\n",
    "\n",
    "word_count_rdd = \"\"\" your word-count \"\"\"\n",
    "\n",
    "# print your result\n",
    "word_count_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `takeOrdered(n, key)` allows to collect only the first $n$ elements based on the ordering provided by key. Ordering is by default ascending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the 10 most occurent words\n",
    "word_count_rdd.\"\"\" your takeOrdered \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orange bar for the first time! because we sort of group by, there we SHUFFLE\\\n",
    "might be the part that take the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: create the words_rdd where each element is a word\n",
    "# eg: ['one', 'ring', 'to', 'rule', 'them']\n",
    "\n",
    "def clean_row(row):\n",
    "    # this function should:\n",
    "    #   1. clean a row from punctuation \n",
    "    #   2. put each word in lowercase\n",
    "    #   3. and return the list of words\n",
    "    word_list = []\n",
    "    \n",
    "    # your code\n",
    "    for word in row.strip().split():\n",
    "        clean_word = ''\n",
    "        #scan the rows looking for punctuation\n",
    "        for char not in string.punctuation:\n",
    "            cle\n",
    "        #lower the words\n",
    "        if clean_word != '':\n",
    "            word_list.append(clean_word.lower())\n",
    "        \n",
    "\n",
    "    \n",
    "    return word_list\n",
    "\n",
    "# apply transformation\n",
    "words_rdd = message_rdd.flatMap(clean)\n",
    "\n",
    "# print a few items\n",
    "words_rdd.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - Distributed linear regression\n",
    "\n",
    "The files in `../datasets/lecture1/` contain a series of measurements of a sensor. Each measure is in the form \n",
    "\n",
    "```\n",
    "Measure  N: (t,val)\n",
    "```\n",
    "\n",
    "There could be pairs such as `(t, None)`, corresponting to missing values. This measure should be removed. \n",
    "\n",
    "We are interested in knowing if there is any sort of relation between `t` and `val`.\n",
    "\n",
    "The first task consits in reading each file and creating an RDD containing as element the pairs `(t,val)`.\n",
    "\n",
    "We can first inspect the first lines of a file to get a sense of the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../datasets/lecture1/file_1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that each file contain millions of measurements and there may be thousands of files. \n",
    "\n",
    "Reading all the files with Python on our local machine (client-side), and then using `parallelize` should be avoided at all costs.\n",
    "This would be a very inefficient way to read the data, as we would have to read all the files in the driver (possibly getting out of memory), then splitting the data in the workers. \n",
    "\n",
    "Instead, we can parallelize the _list of files_ and then read all files in parallel directly from the Spark executors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/mapd-workspace/datasets/lecture1/file_{}.txt'\n",
    "file_list = [base_path.format(i) for i in range(1,5)]\n",
    "\n",
    "files_rdd = sc.parallelize(file_list)\n",
    "files_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "_In this case we could have used the built-in Spark function `textFile`, which takes as input the path to the files and read them directly into an RDD._\n",
    "\n",
    "_To access a file with this function we need to prepend `file://` if the file is in the local file system. \n",
    "If the file is in hadoop the path will be similar to `hdfs://namenode.com:8020/path_to_the_file`. \n",
    "In the same way, if the file is stored on object stores using the `s3` API (such as the one used in CloudVeneto) the path will be `s3://`._\n",
    "\n",
    "_The Spark function `textFile` could be used in this case as the input files are stored in a plain text `.txt` format. In many other cases, however, a custom data loader is required instead, as the data will be stored in a specific format, impossible for Spark to interpret without some instructions. On the other hand, we should try to use the built-in data readout functions as much as possible since they are much more performant than our custom python code._\n",
    "\n",
    "```python\n",
    "text_rdd = sc.textFile(\"file://\"+base_path.format('*'))\n",
    "text_rdd.collect()[:4]\n",
    "```\n",
    "\n",
    "_In this example we will write a custom data loader, that could be generalized to all kinds of files and data formats._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the RDD files, write a map function that convert data into an RDD, `data_rdd`, where each element is a tuple `(t, val)`. \n",
    "\n",
    "During this preprocessing phase remember to remove points with `None` as measure. \n",
    "\n",
    "The elements of `data_rdd` should finally be something like:\n",
    "\n",
    "```\n",
    "[(0.0,9.93), (-1.0,9.02), ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do this, we can use _regular expressions (or regex)_.\n",
    "\n",
    "A regular expression is a sequence of characters that allows to identify and select a **pattern** in a string, instead of a specific substring.\n",
    "Most editors (including Jupyter) allow to use regex to find and replace text based on patterns, and several interesing sites offer a simple interface to learn and experiment with regexp, e.g. https://regex101.com/\n",
    "\n",
    "All following charachters are considered \"meta-characters\" in regex\n",
    "\n",
    "`. ^ $ * + ? { } [ ] \\ | ( )`\n",
    "\n",
    "In a regular expression, all these symbols will be **not** interpreted as their characters, but are used to build up the patterns and search for matches in the text.\n",
    "\n",
    "The main notable examples are:\n",
    "\n",
    "`[ ]` square brakets meaning exactly 1 character\n",
    "\n",
    "| regexp | interpretation |\n",
    "| --- | --- |\n",
    "| \\[a\\] | the letter `a` |\n",
    "| \\[ax\\] | the letter `a` or `x` |\n",
    "| \\[a-x\\] | any letter from `a` to `x` |\n",
    "| \\[a-zA-F\\] | any letter from `a` to `z` and from `A` to `F` |\n",
    "| \\[0-9\\] | any number from 0 to 9 |\n",
    "\n",
    "`^` represents a `not`\n",
    "\n",
    "| regexp | interpretation |\n",
    "| --- | --- |\n",
    "| \\[^b02\\] | all charachters but `b`,`0`and `2` |\n",
    "\n",
    "`.` represents any character at all\n",
    "\n",
    "| regexp | interpretation |\n",
    "| --- | --- |\n",
    "| . | all charachters |\n",
    "\n",
    "`?` makes the preceding character in the regular expression optional\n",
    "\n",
    "| regexp | interpretation |\n",
    "| --- | --- |\n",
    "| colou?r | `color` or `colour` |\n",
    "\n",
    "`*` match the preceding character (or combination) zero or more times\n",
    "\n",
    "| regexp | interpretation |\n",
    "| --- | --- |\n",
    "| go*gle | `ggle`,`gogle`,`google`,`gooogle`,... |\n",
    "\n",
    "`\\t` matches a tab\n",
    "\n",
    "`\\n` matches a new-line\n",
    "\n",
    "`^` (outside of `[]`) matches the start of a line\n",
    "\n",
    "`$` matches the end of a line\n",
    "\n",
    "Regex can be used in plain Python with the `re` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `re` module to implement the appropriate regular expression to clean up our data.\n",
    "\n",
    "We should find all float values possibly starting with a +/- sign and collecting them in pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the regex pattern with a fake measurement\n",
    "import re\n",
    "\n",
    "fake_meas = 'Measure  0: (.05,0.70)'\n",
    "float_pattern = '-?[0-9]*\\.[0-9]*'\n",
    "test_ = re.findall(f'{float_pattern},{float_pattern}', fake_meas)\n",
    "test_   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(file):    \n",
    "    # this function should:\n",
    "    #   1. open the file and read all lines\n",
    "    #   2. apply the regex to extract the values \n",
    "    #   3. sanify the data by removing all measurement pairs with non-numerical values\n",
    "    #   4. create a `points` list where each entry is a tuple of the two values/coordinates\n",
    "    points = []\n",
    "    \n",
    "    # your code\n",
    "\n",
    "    # ---\n",
    "    \n",
    "    return points\n",
    "\n",
    "data_rdd = \"\"\" parallelize the reading and parsing of the measurements \"\"\"\n",
    "data_rdd.collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `sample(with_replacement, fraction)` we can sample some points from the original RDD. \n",
    "\n",
    "This is useful if we want to collect only a given `fraction` of our data, without dumping the entire dataset with a collect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "# collect a sub-sample of your data\n",
    "data = np.array(data_rdd.sample(False,0.2).collect())\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1]);\n",
    "plt.xlabel(r'$p_x$');\n",
    "plt.ylabel(r'$p_y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will now implement a distributed gradient descent and use it to find the best parameters for a linear model.**\n",
    "\n",
    "Given an input $X$ the ouput of the model is \n",
    "\n",
    "$$\n",
    "Y = W X = w_{0} + w_1 x_1 + \\dots + w_p x_p\n",
    "$$\n",
    "\n",
    "In our case, we have $Y=y(w,x)$, $W = [w_0, w_1]$ and $X=[1, x]^T$, in other words\n",
    "\n",
    "$$\n",
    "y(w,x) = w_0 + w_1 x\n",
    "$$\n",
    "\n",
    "We are interested in estimating the optimal parameters $w^\\star$ of the line fitting our data. \n",
    "\n",
    "To do this we will use **gradient descent**, an iterative procedure that allows us to find a local minumum of a differentiable function.\n",
    "\n",
    "In our case, we would like to minimize the square residuals, i.e.\n",
    "\n",
    "$$\n",
    "J(W,X) = \\frac{1}{2n} \\sum_{i=1}^{n} [Y(W, X)- y_i]^2 = \\frac{1}{2n} \\sum_{i=1}^{n} [ (w_0 +w_1x) - y_i]^2\n",
    "$$\n",
    "\n",
    "where $y_i$ is the true value. \n",
    "\n",
    "In each step of the gradient descent we use the following update rule\n",
    "\n",
    "$$\n",
    "W_{i+1} = W_i - \\gamma \\nabla J_W(W_i, X)\n",
    "$$\n",
    "\n",
    "where $\\gamma$ is the learning rate, a variable used to reduce the size of each step.\n",
    "\n",
    "\n",
    "In other word we are moving in the opposite direction of the gradient, i.e. towards the minimum of the function. \n",
    "\n",
    "Recalling that $W=[w_0, w_1]$ and $X=[1,x]$ we have that each component of the gradient, i.e. $\\frac{\\partial}{\\partial w_p} J(W,X) $\n",
    "\n",
    "$$\n",
    "\\nabla J(W, X) = \\left[\\frac{1}{n} \\sum_{i=1}^{n} [Y(W,X)- y_i]\\cdot1, \\frac{1}{n} \\sum_{i=1}^{n} [Y(W,X)- y_i]\\cdot x_i \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can now write a map-reduce job used to estimate the parameters using gradient descent on the full dataset.**\n",
    "\n",
    "We start by defining the weights vector and initializing it to some values, e.g. $(10, 0.5)$ is a good guess..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use numpy to create the W weight array\n",
    "W = \"\"\" a simple numpy array \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement then the functions computing the prediction given as input $x$ and the current weights W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict y for any given x and weights W\n",
    "def predict(x, W):\n",
    "    # return prediction    \n",
    "    \n",
    "    # ---\n",
    "    \n",
    "# test the function in local \n",
    "\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function computing the gradient for one example, given as inputs:\n",
    "1. the point $P=(x,y)$, and \n",
    "2. the current set of weights $W$. \n",
    "\n",
    "Remember that the gradient has two components, one per parameter. \n",
    "\n",
    "Furthermore, the normalization $\\frac{1}{n}$ can be ommited since we can apply it afterwards, having summed the gradients of all examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the gradient \n",
    "def gradient(P, W):\n",
    "    # return the prediction given the x value of P and the weights W\n",
    "    # and compute gradient\n",
    "    \n",
    "    # ---\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "# test the function in local\n",
    "\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to implement the gradient descent and find the optimal line parameters. \n",
    "\n",
    "**Hint**: compute the gradient for each point in parallel, and them sum all of them. Finally, use this sum to update the weights vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count points based on the partitioned data\n",
    "num_points = \"\"\" count the points in the partitioned data \"\"\"\n",
    "\n",
    "# re-declare the weight vector here: \n",
    "#   this is useful if the cell is run multiple times\n",
    "W = \"\"\" a simple numpy array \"\"\"\n",
    "\n",
    "# define the learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# define the number of iterations\n",
    "num_it = 20\n",
    "\n",
    "for i in range(num_it):\n",
    "    # run the gradient descent in parallel and then sum the gradients \n",
    "    grad = \"\"\" the parallelized gradient evaluation \"\"\"\n",
    "    # ---\n",
    "    \n",
    "    # update the weights according to the learning rate and the gradient\n",
    "    W =  \"\"\" the weights update \"\"\"\n",
    "    # ---    \n",
    "    \n",
    "# print the results\n",
    "print(\"Final parameters: x0={:.2f}, x1={:.2f}\".format(W[0], W[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the results by plotting the data points and the resulting best fit line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data_rdd.collect())\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1]);\n",
    "x = np.arange(-10,11)\n",
    "y = W[0] + W[1]*x\n",
    "plt.plot(x, y, color='red', lw=2);\n",
    "plt.xlabel(r'$x_{0}$');\n",
    "plt.ylabel(r'$x_{1}$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finish up by computing the residuals using Spark, and plot their distribution in a histogram. \n",
    "\n",
    "The residual of the point $(x_i, y_i)$ with respect to the model $y(x)$ is simply defined as\n",
    "\n",
    "$$\n",
    "R_i = y(x_i) - y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the residuals\n",
    "residuals_rdd = \"\"\" residual evaluation \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot its distribution\n",
    "plt.hist(residuals_rdd.collect(), range=(-1,1), bins=50);\n",
    "plt.xlabel(\"residuals\");\n",
    "plt.ylabel(\"counts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "_By running the previous cell we asked Spark to return the entire `residual` RDD object. \n",
    "This is not a sizeable dataset in this specific example, but it might be the case in a real-life scenario, where you may have lots of records stored in a distributed cluster.\n",
    "Returning the entire dataset just to produce a histogram is extremely inconvenient, as it implies large data transfers from the executors to the driver, and finally to the client (possibly resulting in getting out of the driver memory)._\n",
    "\n",
    "_What you should be doing instead is to evaluate the histogram remotely and in parallel, and return the histogram as a set of bins and counts.\n",
    "This way the data transfer from the executors to the driver is minimized, avoiding possible issues._\n",
    "\n",
    "_You should be able to write a Map-Reduce function to create an histogram (consider this an **Extra-exercise**), but Spark is also providing some built-in functions to compute simple histograms remotely._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the spark RDD histogram method to return the histogram from the cluster\n",
    "hist = residuals_rdd.histogram(np.linspace(-1,1,50).tolist())\n",
    "plt.stairs(hist[1], hist[0]);\n",
    "plt.xlabel(\"residuals\");\n",
    "plt.ylabel(\"counts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching\n",
    "\n",
    "From the WebUI we can see that in each iteration Spark is computing every operation from the very beginning, i.e. **starting from reading the list of filenames and runnign the parallelization of the text files!**. \n",
    "This would not really necessary, as this first operation should not need to be re-executed every time we need to access the dataset.\n",
    "\n",
    "With `persist()` we can instruct spark to put intermediate results into the executors' memory, e.g. after the function parsing the files. \n",
    "\n",
    "In this way the same dataset will be loaded in the next iterations much faster, at the cost of having the dataset stored in memory. \n",
    "\n",
    "**Note-1:** to be precise, there could be different levels of [persistence](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence) of data into the executors' memory.\n",
    "\n",
    "**Note-2:** one should be very careful not to _abuse_ the caching of datasets. Caching itself takes time and resources, and caching all intermediate stages of your dataset might end up taking a toll on the performance (and saturating the available memory of the executors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist the original RDD\n",
    "data_rdd = data_rdd.persist() \n",
    "\n",
    "# persist is a transformation, it will be triggered \n",
    "# only once we apply an action on the RDD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run the same code as previously \n",
    "# and have a look at the spark application webUI \n",
    "# under the Storage tab\n",
    "\n",
    "print(\"Final parameters: x0={:.2f}, x1={:.2f}\".format(W[0], W[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD can be removed from memory with `unpersist()`, freeing up the executors' memory when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up the memory\n",
    "data_rdd = data_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop worker and master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the running Spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the running Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use `docker compose down` to stop and clear all running containers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
